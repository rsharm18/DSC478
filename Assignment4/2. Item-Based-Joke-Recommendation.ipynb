{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a Load in the joke ratings data and the joke text data into appropriate data structures. Use the \"recommend\" function to provide top 5 joke recommendations for at least 2 users. Use both standard item-based collaborative filtering (based on the rating prediction function \"standEst\") and the SVD-based version of the item-based CF (using \"svdEst\" as the prediction engine) to generate these recommendations for the two users and note the differences. You should show the text of the recommended jokes as well as the predicted ratings for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes = pd.read_csv('data/jokes/jokes.csv',na_values=['?'],delimiter=\",\",header=None)\n",
    "mod_data = pd.read_csv('data/jokes/modified_jester_data.csv',delimiter=\",\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A man visits the doctor. The doctor says \"I have bad news for you.You have cancer and Alzheimer\\'s disease\". The man replies \"Well thank God I don\\'t have cancer!\"'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes.iloc[:,1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.18</td>\n",
       "      <td>19.79</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.48</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.15</td>\n",
       "      <td>15.17</td>\n",
       "      <td>2.02</td>\n",
       "      <td>6.24</td>\n",
       "      <td>...</td>\n",
       "      <td>13.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.08</td>\n",
       "      <td>10.71</td>\n",
       "      <td>17.36</td>\n",
       "      <td>15.37</td>\n",
       "      <td>8.62</td>\n",
       "      <td>1.34</td>\n",
       "      <td>10.27</td>\n",
       "      <td>5.66</td>\n",
       "      <td>19.88</td>\n",
       "      <td>20.22</td>\n",
       "      <td>...</td>\n",
       "      <td>13.82</td>\n",
       "      <td>6.05</td>\n",
       "      <td>10.71</td>\n",
       "      <td>18.86</td>\n",
       "      <td>10.81</td>\n",
       "      <td>8.86</td>\n",
       "      <td>14.06</td>\n",
       "      <td>11.34</td>\n",
       "      <td>6.68</td>\n",
       "      <td>12.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.03</td>\n",
       "      <td>20.27</td>\n",
       "      <td>20.03</td>\n",
       "      <td>20.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>19.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.80</td>\n",
       "      <td>19.16</td>\n",
       "      <td>8.18</td>\n",
       "      <td>17.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.84</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.50</td>\n",
       "      <td>15.61</td>\n",
       "      <td>6.83</td>\n",
       "      <td>5.61</td>\n",
       "      <td>12.36</td>\n",
       "      <td>12.60</td>\n",
       "      <td>18.04</td>\n",
       "      <td>15.61</td>\n",
       "      <td>10.56</td>\n",
       "      <td>16.73</td>\n",
       "      <td>...</td>\n",
       "      <td>16.19</td>\n",
       "      <td>16.58</td>\n",
       "      <td>15.27</td>\n",
       "      <td>16.19</td>\n",
       "      <td>16.73</td>\n",
       "      <td>12.55</td>\n",
       "      <td>14.11</td>\n",
       "      <td>17.55</td>\n",
       "      <td>12.80</td>\n",
       "      <td>12.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>12.94</td>\n",
       "      <td>5.47</td>\n",
       "      <td>16.19</td>\n",
       "      <td>5.51</td>\n",
       "      <td>6.92</td>\n",
       "      <td>8.48</td>\n",
       "      <td>14.20</td>\n",
       "      <td>14.83</td>\n",
       "      <td>4.98</td>\n",
       "      <td>13.96</td>\n",
       "      <td>...</td>\n",
       "      <td>6.58</td>\n",
       "      <td>9.93</td>\n",
       "      <td>15.37</td>\n",
       "      <td>7.89</td>\n",
       "      <td>13.72</td>\n",
       "      <td>6.87</td>\n",
       "      <td>13.23</td>\n",
       "      <td>5.47</td>\n",
       "      <td>14.54</td>\n",
       "      <td>13.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>15.27</td>\n",
       "      <td>11.39</td>\n",
       "      <td>16.39</td>\n",
       "      <td>5.37</td>\n",
       "      <td>7.41</td>\n",
       "      <td>16.58</td>\n",
       "      <td>12.17</td>\n",
       "      <td>2.84</td>\n",
       "      <td>5.13</td>\n",
       "      <td>4.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>16.58</td>\n",
       "      <td>16.63</td>\n",
       "      <td>15.85</td>\n",
       "      <td>7.89</td>\n",
       "      <td>14.40</td>\n",
       "      <td>9.74</td>\n",
       "      <td>14.54</td>\n",
       "      <td>13.14</td>\n",
       "      <td>6.34</td>\n",
       "      <td>11.78</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>3.67</td>\n",
       "      <td>4.45</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>9.40</td>\n",
       "      <td>7.65</td>\n",
       "      <td>3.86</td>\n",
       "      <td>4.40</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.93</td>\n",
       "      <td>...</td>\n",
       "      <td>3.82</td>\n",
       "      <td>6.87</td>\n",
       "      <td>6.87</td>\n",
       "      <td>3.77</td>\n",
       "      <td>3.77</td>\n",
       "      <td>3.77</td>\n",
       "      <td>3.77</td>\n",
       "      <td>3.77</td>\n",
       "      <td>3.77</td>\n",
       "      <td>3.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>9.88</td>\n",
       "      <td>11.73</td>\n",
       "      <td>9.16</td>\n",
       "      <td>9.50</td>\n",
       "      <td>13.52</td>\n",
       "      <td>12.07</td>\n",
       "      <td>14.50</td>\n",
       "      <td>11.97</td>\n",
       "      <td>13.28</td>\n",
       "      <td>11.68</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1      2      3      4      5      6      7      8      9   \\\n",
       "0     3.18  19.79   1.34   2.84   3.48   2.50   1.15  15.17   2.02   6.24   \n",
       "1    15.08  10.71  17.36  15.37   8.62   1.34  10.27   5.66  19.88  20.22   \n",
       "2     0.00   0.00   0.00   0.00  20.03  20.27  20.03  20.27   0.00   0.00   \n",
       "3     0.00  19.35   0.00   0.00  12.80  19.16   8.18  17.21   0.00  12.84   \n",
       "4    19.50  15.61   6.83   5.61  12.36  12.60  18.04  15.61  10.56  16.73   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "995  12.94   5.47  16.19   5.51   6.92   8.48  14.20  14.83   4.98  13.96   \n",
       "996  15.27  11.39  16.39   5.37   7.41  16.58  12.17   2.84   5.13   4.30   \n",
       "997  16.58  16.63  15.85   7.89  14.40   9.74  14.54  13.14   6.34  11.78   \n",
       "998   3.67   4.45   3.67   3.67   9.40   7.65   3.86   4.40   3.67   4.93   \n",
       "999   9.88  11.73   9.16   9.50  13.52  12.07  14.50  11.97  13.28  11.68   \n",
       "\n",
       "     ...     90     91     92     93     94     95     96     97     98     99  \n",
       "0    ...  13.82   0.00   0.00   0.00   0.00   0.00   5.37   0.00   0.00   0.00  \n",
       "1    ...  13.82   6.05  10.71  18.86  10.81   8.86  14.06  11.34   6.68  12.07  \n",
       "2    ...   0.00   0.00   0.00  20.08   0.00   0.00   0.00   0.00   0.00   0.00  \n",
       "3    ...   0.00   0.00   0.00  11.53   0.00   0.00   0.00   0.00   0.00   0.00  \n",
       "4    ...  16.19  16.58  15.27  16.19  16.73  12.55  14.11  17.55  12.80  12.60  \n",
       "..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "995  ...   6.58   9.93  15.37   7.89  13.72   6.87  13.23   5.47  14.54  13.38  \n",
       "996  ...   0.00   0.00   0.00   0.00   6.58   0.00   0.00   0.00   0.00   0.00  \n",
       "997  ...   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  \n",
       "998  ...   3.82   6.87   6.87   3.77   3.77   3.77   3.77   3.77   3.77   3.28  \n",
       "999  ...   0.00   0.00   0.00   0.00  12.12   0.00   0.00   0.00   0.00   0.00  \n",
       "\n",
       "[1000 rows x 100 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidSim(inA,inB):\n",
    "    return 1.0 / (1.0 + la.norm(inA - inB))\n",
    "\n",
    "def pearsonSim(inA,inB):\n",
    "    if len(inA) < 3 : return 1.0\n",
    "    return 0.5 + 0.5 * corrcoef(inA, inB, rowvar = 0)[0][1]\n",
    "\n",
    "def cosineSim(inA,inB):\n",
    "    num = float(inA.T * inB)\n",
    "    denom = la.norm(inA)*la.norm(inB)\n",
    "    return 0.5 + 0.5 * (num / denom)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def standEst(dataMat, user, simMeas, item):\n",
    "    n = shape(dataMat)[1]\n",
    "    simTotal = 0.0; ratSimTotal = 0.0\n",
    "    data=mat(dataMat)\n",
    "    for j in range(n):\n",
    "        userRating = data[user,j]\n",
    "        if userRating == 0: continue\n",
    "        overLap = nonzero(logical_and(data[:,item]>0, data[:,j]>0))[0]\n",
    "        if len(overLap) == 0:\n",
    "            similarity = 0\n",
    "        else:\n",
    "            similarity = simMeas(data[overLap,item], data[overLap,j])\n",
    "        #print('the %d and %d similarity is: %f' % (item, j, similarity))\n",
    "        simTotal += similarity\n",
    "        ratSimTotal += similarity * userRating\n",
    "    if simTotal == 0: return 0\n",
    "    else: return ratSimTotal/simTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as la\n",
    "\n",
    "def svdEst(dataMat, user, simMeas, item):\n",
    "    n = shape(dataMat)[1]\n",
    "    k = 4 #number of dimension for SVD\n",
    "    simTotal = 0.0; ratSimTotal = 0.0\n",
    "    data=mat(dataMat)\n",
    "    U,Sigma,VT = la.svd(data)\n",
    "    Sig_k = mat(eye(k)*Sigma[:k]) #arrange Sig_k into a diagonal matrix\n",
    "    xformedItems = data.T * U[:,:k] * Sig_k.I  #create transformed items\n",
    "    for j in range(n):\n",
    "        userRating = data[user,j]\n",
    "        if userRating == 0 or j==item: continue\n",
    "        similarity = simMeas(xformedItems[item,:].T, xformedItems[j,:].T)\n",
    "        #print('the %d and %d similarity is: %f' % (item, j, similarity))\n",
    "        simTotal += similarity\n",
    "        ratSimTotal += similarity * userRating\n",
    "    if simTotal == 0: return 0\n",
    "    else: return ratSimTotal/simTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def recommend(dataMat, user, N=3, simMeas=pearsonSim, estMethod=standEst):\n",
    "    unratedItems = nonzero(dataMat[user,:].A==0)[1] #find unrated items \n",
    "    if len(unratedItems) == 0: return 'you rated everything'\n",
    "    itemScores = []\n",
    "    for item in unratedItems:\n",
    "        estimatedScore = estMethod(dataMat, user, simMeas, item)\n",
    "        itemScores.append((item, estimatedScore))\n",
    "    return sorted(itemScores, key=lambda jj: jj[1], reverse=True)[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_arr = np.array(jokes.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topNJokes(u,N,simMeas=pearsonSim,estMethod=standEst):\n",
    "    recom_jokes = recommend(data_mat,u,N,pearsonSim,estMethod)\n",
    "    \n",
    "    if(recom_jokes=='you rated everything'):\n",
    "        return;\n",
    "        \n",
    "    print('*********************Top ',N,' jokes for user#',u,' are : ****************************** \\n')\n",
    "    \n",
    "    for i in range(len(recom_jokes)):\n",
    "        print(\"\\t\",\" JOKE :   (\",recom_jokes[i][0],\") - \",jokes_arr[recom_jokes[i][0]],'\\n\\t RATING (Predicted) = ',recom_jokes[i][1],'\\n')\n",
    "        \n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mat = np.mat(mod_data)\n",
    "N=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************Top  5  jokes for user# 0  are : ****************************** \n",
      "\n",
      "\t  JOKE :   ( 71 ) -  On the first day of college the Dean addressed the students pointing out some of the rules:\"The female dormitory will be out-of-bounds for all male students and the male dormitory to the female students. Anybody caught breaking this rule will be fined $20 the first time.\" He continued \"Anybody caught breaking this rule the second time will be fined $60. Being caught a third time will cost you a fine of $180. Are there any questions ?\"At this point a male student in the crowd inquired:\"How much for a season pass ?\" \n",
      "\t RATING (Predicted) =  7.7688045421022185 \n",
      "\n",
      "\t  JOKE :   ( 88 ) -  A radio conversation of a US naval ship with Canadian authorities ... Americans: Please divert your course 15 degrees to the North to avoid a collision.Canadians: Recommend you divert YOUR course 15 degrees to the South to avoid a collision.Americans: This is the Captain of a US Navy ship.  I say again divert YOUR course.Canadians: No.  I say again you divert YOUR course.Americans: This is the aircraft carrier USS LINCOLN the second largest ship in the United States' Atlantic Fleet. We are accompanied by three destroyers three cruisers and numerous support vessels. I demand that you change your course 15 degrees north that's ONE FIVE DEGREES NORTH or counter-measures will be undertaken to ensure the safety of this ship.Canadians: This is a lighthouse.  Your call. \n",
      "\t RATING (Predicted) =  7.735763815113254 \n",
      "\n",
      "\t  JOKE :   ( 99 ) -  Q: What's the difference between greeting a Queen and greeting thePresident of the United  States?A: You only have to get on one knee to greet the queen. \n",
      "\t RATING (Predicted) =  7.668980158526493 \n",
      "\n",
      "\t  JOKE :   ( 97 ) -  Age and Womanhood1. Between the ages of 13 and 18 ... She is like Africa virgin and unexplored. 2. Between the ages of 19 and 35 ... She is like Asia hot and exotic. 3. Between the ages of 36 and 45 ... She is like America fully explored breathtakingly beautiful and free with her resources.4. Between the ages of 46 and 56 ...She is like Europe exhausted but still has points of interest. 5. After 56 she is like Australia ...Everybody knows it's down there but who gives a damn? \n",
      "\t RATING (Predicted) =  7.665838968203219 \n",
      "\n",
      "\t  JOKE :   ( 92 ) -  Reaching the end of a job interview the human resources person asked a young engineer fresh out of Stanford\"And what starting salary were you looking for?\"The engineer said \"In the neighborhood of $125000 a year depending on the benefits package.\"The interviewer said \"Well what would you say to a package of 5-weeks vacation 14 paid holidays full medical and dental company matching retirement fund to 50% of salary and a company car leased every 2 years - say a red Corvette?\"The Engineer sat up straight and said \"Wow! Are you kidding?\"And the interviewer replied \"Yeah but you started it.\" \n",
      "\t RATING (Predicted) =  7.649583358689659 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*********************Top  5  jokes for user# 2  are : ****************************** \n",
      "\n",
      "\t  JOKE :   ( 86 ) -  A man recently completing a routine physical examination receives a phone call from his doctor.  The doctor says \"I have some good news and some bad news.\"  The man says \"OK give me the good news first.\"  The doctor says \"The good news is you have 24 hours to live.\"  The man replies \"Shit!  That's the good news?  Then what's the bad news?\"The doctor says \"The bad news is I forgot to call you yesterday.\" \n",
      "\t RATING (Predicted) =  18.241650140258795 \n",
      "\n",
      "\t  JOKE :   ( 92 ) -  Reaching the end of a job interview the human resources person asked a young engineer fresh out of Stanford\"And what starting salary were you looking for?\"The engineer said \"In the neighborhood of $125000 a year depending on the benefits package.\"The interviewer said \"Well what would you say to a package of 5-weeks vacation 14 paid holidays full medical and dental company matching retirement fund to 50% of salary and a company car leased every 2 years - say a red Corvette?\"The Engineer sat up straight and said \"Wow! Are you kidding?\"And the interviewer replied \"Yeah but you started it.\" \n",
      "\t RATING (Predicted) =  18.220152534356433 \n",
      "\n",
      "\t  JOKE :   ( 71 ) -  On the first day of college the Dean addressed the students pointing out some of the rules:\"The female dormitory will be out-of-bounds for all male students and the male dormitory to the female students. Anybody caught breaking this rule will be fined $20 the first time.\" He continued \"Anybody caught breaking this rule the second time will be fined $60. Being caught a third time will cost you a fine of $180. Are there any questions ?\"At this point a male student in the crowd inquired:\"How much for a season pass ?\" \n",
      "\t RATING (Predicted) =  18.219854289318988 \n",
      "\n",
      "\t  JOKE :   ( 29 ) -  Q: What's the difference between a Lawyer and a Plumber? A: A Plumber works to unclog the system. \n",
      "\t RATING (Predicted) =  18.21649148069107 \n",
      "\n",
      "\t  JOKE :   ( 83 ) -  Q: What is the difference between Mechanical Engineers and Civil Engineers? A: Mechanical Engineers build weapons Civil Engineers build targets. \n",
      "\t RATING (Predicted) =  18.2114013245739 \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#data_mat[0,:]\n",
    "\n",
    "for u in range(3):\n",
    "    topNJokes(u,N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mat[2,83]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************Top  5  jokes for user# 0  are : ****************************** \n",
      "\n",
      "\t  JOKE :   ( 71 ) -  On the first day of college the Dean addressed the students pointing out some of the rules:\"The female dormitory will be out-of-bounds for all male students and the male dormitory to the female students. Anybody caught breaking this rule will be fined $20 the first time.\" He continued \"Anybody caught breaking this rule the second time will be fined $60. Being caught a third time will cost you a fine of $180. Are there any questions ?\"At this point a male student in the crowd inquired:\"How much for a season pass ?\" \n",
      "\t RATING (Predicted) =  7.7688045421022185 \n",
      "\n",
      "\t  JOKE :   ( 88 ) -  A radio conversation of a US naval ship with Canadian authorities ... Americans: Please divert your course 15 degrees to the North to avoid a collision.Canadians: Recommend you divert YOUR course 15 degrees to the South to avoid a collision.Americans: This is the Captain of a US Navy ship.  I say again divert YOUR course.Canadians: No.  I say again you divert YOUR course.Americans: This is the aircraft carrier USS LINCOLN the second largest ship in the United States' Atlantic Fleet. We are accompanied by three destroyers three cruisers and numerous support vessels. I demand that you change your course 15 degrees north that's ONE FIVE DEGREES NORTH or counter-measures will be undertaken to ensure the safety of this ship.Canadians: This is a lighthouse.  Your call. \n",
      "\t RATING (Predicted) =  7.735763815113254 \n",
      "\n",
      "\t  JOKE :   ( 99 ) -  Q: What's the difference between greeting a Queen and greeting thePresident of the United  States?A: You only have to get on one knee to greet the queen. \n",
      "\t RATING (Predicted) =  7.668980158526493 \n",
      "\n",
      "\t  JOKE :   ( 97 ) -  Age and Womanhood1. Between the ages of 13 and 18 ... She is like Africa virgin and unexplored. 2. Between the ages of 19 and 35 ... She is like Asia hot and exotic. 3. Between the ages of 36 and 45 ... She is like America fully explored breathtakingly beautiful and free with her resources.4. Between the ages of 46 and 56 ...She is like Europe exhausted but still has points of interest. 5. After 56 she is like Australia ...Everybody knows it's down there but who gives a damn? \n",
      "\t RATING (Predicted) =  7.665838968203219 \n",
      "\n",
      "\t  JOKE :   ( 92 ) -  Reaching the end of a job interview the human resources person asked a young engineer fresh out of Stanford\"And what starting salary were you looking for?\"The engineer said \"In the neighborhood of $125000 a year depending on the benefits package.\"The interviewer said \"Well what would you say to a package of 5-weeks vacation 14 paid holidays full medical and dental company matching retirement fund to 50% of salary and a company car leased every 2 years - say a red Corvette?\"The Engineer sat up straight and said \"Wow! Are you kidding?\"And the interviewer replied \"Yeah but you started it.\" \n",
      "\t RATING (Predicted) =  7.649583358689659 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*********************Top  5  jokes for user# 2  are : ****************************** \n",
      "\n",
      "\t  JOKE :   ( 86 ) -  A man recently completing a routine physical examination receives a phone call from his doctor.  The doctor says \"I have some good news and some bad news.\"  The man says \"OK give me the good news first.\"  The doctor says \"The good news is you have 24 hours to live.\"  The man replies \"Shit!  That's the good news?  Then what's the bad news?\"The doctor says \"The bad news is I forgot to call you yesterday.\" \n",
      "\t RATING (Predicted) =  18.241650140258795 \n",
      "\n",
      "\t  JOKE :   ( 92 ) -  Reaching the end of a job interview the human resources person asked a young engineer fresh out of Stanford\"And what starting salary were you looking for?\"The engineer said \"In the neighborhood of $125000 a year depending on the benefits package.\"The interviewer said \"Well what would you say to a package of 5-weeks vacation 14 paid holidays full medical and dental company matching retirement fund to 50% of salary and a company car leased every 2 years - say a red Corvette?\"The Engineer sat up straight and said \"Wow! Are you kidding?\"And the interviewer replied \"Yeah but you started it.\" \n",
      "\t RATING (Predicted) =  18.220152534356433 \n",
      "\n",
      "\t  JOKE :   ( 71 ) -  On the first day of college the Dean addressed the students pointing out some of the rules:\"The female dormitory will be out-of-bounds for all male students and the male dormitory to the female students. Anybody caught breaking this rule will be fined $20 the first time.\" He continued \"Anybody caught breaking this rule the second time will be fined $60. Being caught a third time will cost you a fine of $180. Are there any questions ?\"At this point a male student in the crowd inquired:\"How much for a season pass ?\" \n",
      "\t RATING (Predicted) =  18.219854289318988 \n",
      "\n",
      "\t  JOKE :   ( 29 ) -  Q: What's the difference between a Lawyer and a Plumber? A: A Plumber works to unclog the system. \n",
      "\t RATING (Predicted) =  18.21649148069107 \n",
      "\n",
      "\t  JOKE :   ( 83 ) -  Q: What is the difference between Mechanical Engineers and Civil Engineers? A: Mechanical Engineers build weapons Civil Engineers build targets. \n",
      "\t RATING (Predicted) =  18.2114013245739 \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_mat = np.mat(mod_data)\n",
    "\n",
    "for u in range(3):\n",
    "    topNJokes(u,N,svdEst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mat[0,78]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation = recommend(data_mat,0,N,pearsonSim,svdEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(71, 7.585854912512737),\n",
       " (79, 7.468800594157777),\n",
       " (70, 7.441627451919135),\n",
       " (88, 7.404666679814133),\n",
       " (78, 7.40359554406075)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b Complete the definition for the function \"test\". This function iterates over all users and for each performs evaluation (by calling the provided \"cross_validate_user\" function), and returns the error information necessary to compute Mean Absolute Error (MAE). Use this function to perform evaluation (with 20% test-ratio for each user) comparing MAE results using the rating prediction function \"standEst\" with results using the \"svdEst\" prediction function. [Note: See comments provided in the module for hints on accomplishing these tasks.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function performs evaluation on a single user based on the test_ratio\n",
    "# For example, with test_ratio = 0.2, a randomly selected 20 percent of rated \n",
    "# items by the user are withheld as test data. The remaining part of the user\n",
    "# profile is used as input for the estimation functions to predict the \n",
    "# withheld ratings and compute the error for this user\n",
    "\n",
    "def cross_validate_user(dataMat, user, test_ratio, estMethod=standEst, simMeas=pearsonSim):\n",
    "\tdataMat = np.array(dataMat)\n",
    "\tnumber_of_items = np.shape(dataMat)[1]\n",
    "\trated_items_by_user = np.array([i for i in range(number_of_items) if dataMat[user,i]>0])\n",
    "\ttest_size = int(test_ratio * len(rated_items_by_user))\n",
    "\ttest_indices = np.random.randint(0, len(rated_items_by_user), test_size)\n",
    "\twithheld_items = rated_items_by_user[test_indices]\n",
    "\toriginal_user_profile = np.copy(dataMat[user])\n",
    "\tdataMat[user, withheld_items] = 0 # So that the withheld test items is not used in the rating estimation below\n",
    "\terror_u = 0.0\n",
    "\tcount_u = len(withheld_items)\n",
    "\n",
    "\t# Compute absolute error for user u over all test items\n",
    "\tfor item in withheld_items:\n",
    "\t\t# Estimate rating on the withheld item\n",
    "\t\testimatedScore = estMethod(dataMat, user, simMeas, item)\n",
    "\t\terror_u = error_u + abs(estimatedScore - original_user_profile[item])\t\n",
    "\t\n",
    "\t# Now restore ratings of the withheld items to the user profile\n",
    "\tfor item in withheld_items:\n",
    "\t\tdataMat[user, item] = original_user_profile[item]\n",
    "\t\t\n",
    "\t# Return sum of absolute errors and the count of test cases for this user\n",
    "\t# Note that these will have to be accumulated for each user to compute MAE\n",
    "\treturn error_u, count_u\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataMat, test_ratio, estMethod, simMeas=pearsonSim):\n",
    "    number_of_users = np.shape(data_mat)[0]\n",
    "    total_error = 0\n",
    "    total_usecase=0\n",
    "    for i in range(number_of_users):\n",
    "        error_u, count_u = cross_validate_user(dataMat,i,test_ratio)\n",
    "        total_error += error_u;\n",
    "        total_usecase += count_u\n",
    "        \n",
    "    MAE = total_error/total_usecase\n",
    "    print ('Mean Absoloute Error for ',estMethod,' : ', MAE)\n",
    "        \n",
    "    return MAE\n",
    "    # Write this function to iterate over all users and for each perform evaluation by calling\n",
    "\t# the above cross_validate_user function on each user. MAE will be the ratio of total error \n",
    "\t# across all test cases to the total number of test cases, across all users\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absoloute Error for  <function standEst at 0x000001889EF49E50>  :  3.7235014034358382\n",
      "Wall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MAE = test(data_mat,0.2,standEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from test is 3.7235014034358382\n"
     ]
    }
   ],
   "source": [
    "print('MAE from test is',MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.c Write a new function \"print_most_similar_jokes\" which takes the joke ratings data, a query joke id, a parameter k for the number similar jokes, and a similarity metric function, and prints the text of the query joke as well as the texts of the top k most similar jokes based on user ratings. [Note: For hints on how to accomplish this task, please see comments at the end of the provided module as well as comments for the provided stub function.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_search(query_ratings_vector, dataMat, k, metric=cosineSim):\n",
    "    ratings_df = pd.DataFrame(query_ratings_vector)\n",
    "    data = pd.DataFrame(dataMat)\n",
    "#     print(ratings_df.shape)\n",
    "#     print()\n",
    "#     print(data.shape)\n",
    "    \n",
    "    distances = []\n",
    "    # comapre each joke from data with the rating vector\n",
    "    for i in range(dataMat.shape[1]):\n",
    "        data_rating_vector = dataMat[:,i]\n",
    "        distances.append(metric(query_ratings_vector, data_rating_vector))\n",
    "    \n",
    "    \n",
    "    distances_arr = np.array(distances);\n",
    "    #print('distances array shape ',distances_arr.shape)\n",
    "    distances_arr = distances_arr.argsort()\n",
    "    \n",
    "    kNeighbors = zeros((k,dataMat.shape[1]))\n",
    "    topIndicies = []\n",
    "    classCount={}\n",
    "    for i in range(k):\n",
    "        #voteIlabel = labels[sortedDistIndicies[i]]\n",
    "        kNeighbors[i,:] = dataMat[distances_arr[i],:]\n",
    "        topIndicies.append(distances_arr[i])\n",
    "        \n",
    "    return kNeighbors, topIndicies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_similar_jokes(dataMat, jokes, queryJoke, k, metric=pearsonSim):\n",
    "    # Write this function to find the k most similar jokes (based on user ratings) to a queryJoke\n",
    "    # The queryJoke is a joke id as given in the 'jokes.csv' file (an corresponding to the a column in dataMat)\n",
    "    # You must compare ratings for the queryJoke (the column in dataMat corresponding to the joke), to all\n",
    "    # other joke rating vectors and return the top k. Note that this is the same as performing KNN on the \n",
    "    # columns of dataMat. The function must retrieve the text of the joke from 'jokes.csv' file and print both\n",
    "    # the queryJoke text as well as the text of the returned top-k jokes.\n",
    "    \n",
    "    print(' Query joke id',queryJoke, '\\n\\n query joke = \\n   ',jokes[queryJoke])\n",
    "    ##print(' Ratings\\n ',dataMat[:,queryJoke])\n",
    "    kNeighbors, topIndicies = knn_search(dataMat[:,queryJoke], dataMat, k , metric)\n",
    "    \n",
    "    print (\"\\n\\nThe top %d recommended jokes are: \"%(k))\n",
    "    for ind in topIndicies:\n",
    "        jok = jokes[ind]\n",
    "        print()\n",
    "        print (\"#\",ind,\" => \",jok)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Query joke id 97 \n",
      "\n",
      " query joke = \n",
      "    Age and Womanhood1. Between the ages of 13 and 18 ... She is like Africa virgin and unexplored. 2. Between the ages of 19 and 35 ... She is like Asia hot and exotic. 3. Between the ages of 36 and 45 ... She is like America fully explored breathtakingly beautiful and free with her resources.4. Between the ages of 46 and 56 ...She is like Europe exhausted but still has points of interest. 5. After 56 she is like Australia ...Everybody knows it's down there but who gives a damn?\n",
      "\n",
      "\n",
      "The top 10 recommended jokes are: \n",
      "\n",
      "# 15  =>  Q. What is orange and sounds like a parrot?  A. A carrot.\n",
      "\n",
      "# 14  =>  Q:  What did the blind person say when given some matzah?A:  Who the hell wrote this?\n",
      "\n",
      "# 7  =>  Q. Did you hear about the dyslexic devil worshipper? A. He sold his soul to Santa.\n",
      "\n",
      "# 12  =>  They asked the Japanese visitor if they have elections in his country.  \"Every Morning\" he answers.\n",
      "\n",
      "# 19  =>  What's the difference between a MacIntosh and anEtch-A-Sketch? You don't have to shake the Mac to clear the screen. \n",
      "\n",
      "# 6  =>  How many feminists does it take to screw in a light bulb?That's not funny.\n",
      "\n",
      "# 4  =>  Q. What's O. J. Simpson's Internet address? A.\tSlash slash backslash slash slash escape.\n",
      "\n",
      "# 18  =>  Q: If a person who speaks three languages is called \"tri-lingual\" and a person who speaks two languages is called \"bi-lingual\" what do calla person who only speaks one language?A: American! \n",
      "\n",
      "# 17  =>  A dog walks into Western Union and asks the clerk to send a telegram. He fills out a form on which he writes down the telegram he wishes to send: \"Bow wow wow Bow wow wow.\"The clerk says \"You can add another 'Bow wow' for the same price.\"The dog responded \"Now wouldn't that sound a little silly?\" \n",
      "\n",
      "# 16  =>  How many men does it take to screw in a light bulb? One...men will screw anything. \n"
     ]
    }
   ],
   "source": [
    "print_most_similar_jokes(data_mat,jokes_arr,97,10,cosineSim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.d Develop your own item-based collaborative filtering recommender that uses a model-based approach (separating the training and the prediction tasks). In the training component, item-item similarities for all pairs of items are computed and stored in an appropriate data structure. Your training function should be able to use different similarity functions (passed as a parameter) including Cosine Similarity or Pearson Correlation. The prediction (or estimation) function should take as parameters a target user, an item, a value of k, and the similarities data structure and return the predicted rating on the target item for the target user. The predicted rating should be based on the weighted average of the target user's ratings on k most similar items to the target item. You should test the prediction accuracy of your estimation function (using a cross-validation similar to part b, above) and provide a plot of cross-validation accuracies across a range of values of k. Using the best value of k, demonstrate the functionality of your recommender by generating recommendations for several anecdotal users (similar to part a, above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes = pd.read_csv('data/jokes/jokes.csv',na_values=['?'],delimiter=\",\",header=None)\n",
    "jokes_rating = pd.read_csv('data/jokes/modified_jester_data.csv',delimiter=\",\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " jokes  (100, 2)  \n",
      " jokes rating  (1000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\" jokes \",jokes.shape,\" \\n jokes rating \",jokes_rating.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.18</td>\n",
       "      <td>19.79</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.48</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.15</td>\n",
       "      <td>15.17</td>\n",
       "      <td>2.02</td>\n",
       "      <td>6.24</td>\n",
       "      <td>...</td>\n",
       "      <td>13.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.08</td>\n",
       "      <td>10.71</td>\n",
       "      <td>17.36</td>\n",
       "      <td>15.37</td>\n",
       "      <td>8.62</td>\n",
       "      <td>1.34</td>\n",
       "      <td>10.27</td>\n",
       "      <td>5.66</td>\n",
       "      <td>19.88</td>\n",
       "      <td>20.22</td>\n",
       "      <td>...</td>\n",
       "      <td>13.82</td>\n",
       "      <td>6.05</td>\n",
       "      <td>10.71</td>\n",
       "      <td>18.86</td>\n",
       "      <td>10.81</td>\n",
       "      <td>8.86</td>\n",
       "      <td>14.06</td>\n",
       "      <td>11.34</td>\n",
       "      <td>6.68</td>\n",
       "      <td>12.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.03</td>\n",
       "      <td>20.27</td>\n",
       "      <td>20.03</td>\n",
       "      <td>20.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>19.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.80</td>\n",
       "      <td>19.16</td>\n",
       "      <td>8.18</td>\n",
       "      <td>17.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.84</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.50</td>\n",
       "      <td>15.61</td>\n",
       "      <td>6.83</td>\n",
       "      <td>5.61</td>\n",
       "      <td>12.36</td>\n",
       "      <td>12.60</td>\n",
       "      <td>18.04</td>\n",
       "      <td>15.61</td>\n",
       "      <td>10.56</td>\n",
       "      <td>16.73</td>\n",
       "      <td>...</td>\n",
       "      <td>16.19</td>\n",
       "      <td>16.58</td>\n",
       "      <td>15.27</td>\n",
       "      <td>16.19</td>\n",
       "      <td>16.73</td>\n",
       "      <td>12.55</td>\n",
       "      <td>14.11</td>\n",
       "      <td>17.55</td>\n",
       "      <td>12.80</td>\n",
       "      <td>12.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1      2      3      4      5      6      7      8      9   ...  \\\n",
       "0   3.18  19.79   1.34   2.84   3.48   2.50   1.15  15.17   2.02   6.24  ...   \n",
       "1  15.08  10.71  17.36  15.37   8.62   1.34  10.27   5.66  19.88  20.22  ...   \n",
       "2   0.00   0.00   0.00   0.00  20.03  20.27  20.03  20.27   0.00   0.00  ...   \n",
       "3   0.00  19.35   0.00   0.00  12.80  19.16   8.18  17.21   0.00  12.84  ...   \n",
       "4  19.50  15.61   6.83   5.61  12.36  12.60  18.04  15.61  10.56  16.73  ...   \n",
       "\n",
       "      90     91     92     93     94     95     96     97     98     99  \n",
       "0  13.82   0.00   0.00   0.00   0.00   0.00   5.37   0.00   0.00   0.00  \n",
       "1  13.82   6.05  10.71  18.86  10.81   8.86  14.06  11.34   6.68  12.07  \n",
       "2   0.00   0.00   0.00  20.08   0.00   0.00   0.00   0.00   0.00   0.00  \n",
       "3   0.00   0.00   0.00  11.53   0.00   0.00   0.00   0.00   0.00   0.00  \n",
       "4  16.19  16.58  15.27  16.19  16.73  12.55  14.11  17.55  12.80  12.60  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.818540</td>\n",
       "      <td>9.506140</td>\n",
       "      <td>7.65257</td>\n",
       "      <td>5.760030</td>\n",
       "      <td>11.311120</td>\n",
       "      <td>9.398870</td>\n",
       "      <td>10.862890</td>\n",
       "      <td>10.487440</td>\n",
       "      <td>6.623080</td>\n",
       "      <td>10.141500</td>\n",
       "      <td>...</td>\n",
       "      <td>4.668510</td>\n",
       "      <td>4.398860</td>\n",
       "      <td>5.015420</td>\n",
       "      <td>4.463920</td>\n",
       "      <td>4.474250</td>\n",
       "      <td>4.697630</td>\n",
       "      <td>4.840930</td>\n",
       "      <td>4.32064</td>\n",
       "      <td>4.519580</td>\n",
       "      <td>4.34171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.016566</td>\n",
       "      <td>6.813393</td>\n",
       "      <td>7.04054</td>\n",
       "      <td>6.255626</td>\n",
       "      <td>5.019201</td>\n",
       "      <td>7.105225</td>\n",
       "      <td>5.357238</td>\n",
       "      <td>4.804446</td>\n",
       "      <td>6.786949</td>\n",
       "      <td>6.693369</td>\n",
       "      <td>...</td>\n",
       "      <td>6.976533</td>\n",
       "      <td>6.601401</td>\n",
       "      <td>7.115623</td>\n",
       "      <td>6.672744</td>\n",
       "      <td>6.629486</td>\n",
       "      <td>6.762919</td>\n",
       "      <td>6.892321</td>\n",
       "      <td>6.52040</td>\n",
       "      <td>6.380153</td>\n",
       "      <td>6.70066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.490000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.427500</td>\n",
       "      <td>6.680000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.170000</td>\n",
       "      <td>10.685000</td>\n",
       "      <td>6.97000</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>11.870000</td>\n",
       "      <td>10.900000</td>\n",
       "      <td>11.290000</td>\n",
       "      <td>10.760000</td>\n",
       "      <td>4.905000</td>\n",
       "      <td>11.490000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.842500</td>\n",
       "      <td>15.420000</td>\n",
       "      <td>14.20000</td>\n",
       "      <td>10.950000</td>\n",
       "      <td>14.942500</td>\n",
       "      <td>15.660000</td>\n",
       "      <td>15.042500</td>\n",
       "      <td>14.072500</td>\n",
       "      <td>12.525000</td>\n",
       "      <td>15.722500</td>\n",
       "      <td>...</td>\n",
       "      <td>10.560000</td>\n",
       "      <td>9.450000</td>\n",
       "      <td>12.120000</td>\n",
       "      <td>10.130000</td>\n",
       "      <td>9.702500</td>\n",
       "      <td>10.820000</td>\n",
       "      <td>11.062500</td>\n",
       "      <td>9.26250</td>\n",
       "      <td>10.320000</td>\n",
       "      <td>9.80000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.370000</td>\n",
       "      <td>20.320000</td>\n",
       "      <td>20.32000</td>\n",
       "      <td>20.370000</td>\n",
       "      <td>20.370000</td>\n",
       "      <td>20.370000</td>\n",
       "      <td>20.370000</td>\n",
       "      <td>20.320000</td>\n",
       "      <td>20.370000</td>\n",
       "      <td>20.320000</td>\n",
       "      <td>...</td>\n",
       "      <td>20.270000</td>\n",
       "      <td>20.270000</td>\n",
       "      <td>20.320000</td>\n",
       "      <td>20.320000</td>\n",
       "      <td>20.320000</td>\n",
       "      <td>20.270000</td>\n",
       "      <td>20.370000</td>\n",
       "      <td>20.37000</td>\n",
       "      <td>20.320000</td>\n",
       "      <td>20.27000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1           2            3            4   \\\n",
       "count  1000.000000  1000.000000  1000.00000  1000.000000  1000.000000   \n",
       "mean      8.818540     9.506140     7.65257     5.760030    11.311120   \n",
       "std       7.016566     6.813393     7.04054     6.255626     5.019201   \n",
       "min       0.000000     0.000000     0.00000     0.000000     0.000000   \n",
       "25%       0.000000     2.210000     0.00000     0.000000     7.490000   \n",
       "50%      10.170000    10.685000     6.97000     3.280000    11.870000   \n",
       "75%      14.842500    15.420000    14.20000    10.950000    14.942500   \n",
       "max      20.370000    20.320000    20.32000    20.370000    20.370000   \n",
       "\n",
       "                5            6            7            8            9   ...  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  ...   \n",
       "mean      9.398870    10.862890    10.487440     6.623080    10.141500  ...   \n",
       "std       7.105225     5.357238     4.804446     6.786949     6.693369  ...   \n",
       "min       0.000000     1.050000     1.150000     0.000000     0.000000  ...   \n",
       "25%       0.000000     6.427500     6.680000     0.000000     4.010000  ...   \n",
       "50%      10.900000    11.290000    10.760000     4.905000    11.490000  ...   \n",
       "75%      15.660000    15.042500    14.072500    12.525000    15.722500  ...   \n",
       "max      20.370000    20.370000    20.320000    20.370000    20.320000  ...   \n",
       "\n",
       "                90           91           92           93           94  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      4.668510     4.398860     5.015420     4.463920     4.474250   \n",
       "std       6.976533     6.601401     7.115623     6.672744     6.629486   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%      10.560000     9.450000    12.120000    10.130000     9.702500   \n",
       "max      20.270000    20.270000    20.320000    20.320000    20.320000   \n",
       "\n",
       "                95           96          97           98          99  \n",
       "count  1000.000000  1000.000000  1000.00000  1000.000000  1000.00000  \n",
       "mean      4.697630     4.840930     4.32064     4.519580     4.34171  \n",
       "std       6.762919     6.892321     6.52040     6.380153     6.70066  \n",
       "min       0.000000     0.000000     0.00000     0.000000     0.00000  \n",
       "25%       0.000000     0.000000     0.00000     0.000000     0.00000  \n",
       "50%       0.000000     0.000000     0.00000     0.000000     0.00000  \n",
       "75%      10.820000    11.062500     9.26250    10.320000     9.80000  \n",
       "max      20.270000    20.370000    20.37000    20.320000    20.27000  \n",
       "\n",
       "[8 rows x 100 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes_rating.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.d [Extra Credit]: Develop your own item-based collaborative filtering recommender that uses a model-based approach (separating the training and the prediction tasks). In the training component, item-item similarities for all pairs of items are computed and stored in an appropriate data structure. Your training function should be able to use different similarity functions (passed as a parameter) including Cosine Similarity or Pearson Correlation. The prediction (or estimation) function should take as parameters a target user, an item, a value of k, and the similarities data structure and return the predicted rating on the target item for the target user. The predicted rating should be based on the weighted average of the target user's ratings on k most similar items to the target item. You should test the prediction accuracy of your estimation function (using a cross-validation similar to part b, above) and provide a plot of cross-validation accuracies across a range of values of k. Using the best value of k, demonstrate the functionality of your recommender by generating recommendations for several anecdotal users (similar to part a, above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " classes  (100,)  \n",
      " vs_matrix rating  (1000, 100)\n"
     ]
    }
   ],
   "source": [
    "vs_matrix = np.mat(jokes_rating)\n",
    "classes = np.array(jokes.iloc[:,1])\n",
    "\n",
    "print(\" classes \",classes.shape,\" \\n vs_matrix rating \",vs_matrix.shape)\n",
    "\n",
    "##print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(item_index, dataMat, metric=cosineSim):\n",
    "    query_ratings_vector = dataMat[:,item_index]\n",
    "    ratings_df = pd.DataFrame(query_ratings_vector)\n",
    "    data = pd.DataFrame(dataMat)\n",
    "#     print(ratings_df.shape)\n",
    "#     print()\n",
    "#     print(data.shape)\n",
    "    \n",
    "    distances = {}\n",
    "    distance_arr = []\n",
    "    similar_item_arr = []\n",
    "    count = 0\n",
    "    # comapre each joke from data with the rating vector\n",
    "    for i in range(dataMat.shape[1]):\n",
    "        \n",
    "        ## if current item is same as input item then set the sim as 0\n",
    "        if (i==item_index):\n",
    "            distance_arr.append(0)\n",
    "        else:\n",
    "            data_rating_vector = dataMat[:,i]\n",
    "            distance_arr.append(metric(query_ratings_vector, data_rating_vector))\n",
    "        \n",
    "        similar_item_arr.append(i)\n",
    "    distances[item_index] = { 'distance' : distance_arr, 'items' : similar_item_arr}\n",
    "    \n",
    "    return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_model(dataMat,metric=cosineSim):\n",
    "    distances = {}\n",
    "    for i in range(dataMat.shape[1]):\n",
    "        distances.update(training_model(i,dataMat,metric))\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 305 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vs_matrix = np.mat(jokes_rating)\n",
    "\n",
    "tpercent = 0.8\n",
    "tsize = int(tpercent * len(vs_matrix))\n",
    "\n",
    "training_data_set = vs_matrix[:tsize,:] # 80% training data\n",
    "test_data_set = vs_matrix[tsize:,:] # 20% training data\n",
    "\n",
    "## train model for training data\n",
    "values = prepare_training_model(training_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## values is a map with key as items and values as an array of distance from each of the other items\n",
    "values.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shpae of distances for item 1 from other items\n",
    "np.array(values[1]['distance']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50, 74], dtype=int64)"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance = values[50]['distance']\n",
    "#sort the distances    \n",
    "idx = np.argsort(distance)\n",
    "\n",
    "## get top k similar items\n",
    "top_k = idx[:2]\n",
    "top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(user,item,k,ds):\n",
    "    user_df = pd.DataFrame(user)\n",
    "    \n",
    "    if(k==0):\n",
    "        return np.array(user_df[item])[0],[]\n",
    "    \n",
    "    ## get the distance info for the item\n",
    "    distance = ds[item]['distance']\n",
    "    \n",
    "    #sort the distances    \n",
    "    idx = np.argsort(distance)\n",
    "    \n",
    "    ## get top k similar items\n",
    "    top_k = idx[:k]\n",
    "    \n",
    "    #print(\"top_k similar items \",top_k)\n",
    "    \n",
    "    ratings = np.array(user_df[top_k])\n",
    "    \n",
    "    #print(\"input user's ratings for similar items \",ratings)\n",
    "    \n",
    "    dist = sum(ratings)/k\n",
    "    \n",
    "#     print(ratings/k)\n",
    "#     dist = 0\n",
    "#     for i in range(len(top_k)):\n",
    "#         #print(\"\\n top_k - i \",i,\" - \",top_k[i])\n",
    "#         arr = np.array(values[0]['distance'])\n",
    "        \n",
    "#         ## get the average rating of the simialr item(s) \n",
    "#         dist += sum(arr)/(arr.shape[0])\n",
    "        \n",
    "#     #print(\" distance \",dist, \" average \",(dist/k) )\n",
    "    \n",
    "#     ## return the weighted average rating\n",
    "#     return dist/k , top_k\n",
    "    return dist,top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_rating  2.928\n",
      "item :  50  original rating  0.0  predicted rating  2.928\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=2,suppress=True)\n",
    "\n",
    "item = 50\n",
    "K = 5\n",
    "user_number = 100\n",
    "user = test_data_set[user_number]\n",
    "\n",
    "#print(np.ravel(user)[item])\n",
    "pred_rating,top_k = test(user,item,K,values)\n",
    "print(\"pred_rating \",pred_rating)\n",
    "\n",
    "user_updated=np.ravel(user).copy()\n",
    "user_updated[item] = pred_rating\n",
    "print('item : ',item,' original rating ',np.ravel(user)[item],' predicted rating ', user_updated[item],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs evaluation on a single user based on the test_ratio\n",
    "# For example, with test_ratio = 0.2, a randomly selected 20 percent of rated \n",
    "# items by the user are withheld as test data. The remaining part of the user\n",
    "# profile is used as input for the estimation functions to predict the \n",
    "# withheld ratings and compute the error for this user\n",
    "\n",
    "def cross_validate_user_modified(dataMat, user, predicted_rating,item):\n",
    "    dataMat = np.array(dataMat)\n",
    "    original_user_profile = np.copy(dataMat[user])\n",
    "    original_user_rating = original_user_profile[item]\n",
    "    error_u = abs(original_user_rating - predicted_rating)\n",
    "    # Return sum of absolute errors and the count of test cases for this user\n",
    "    # Note that these will have to be accumulated for each user to compute MAE\n",
    "    return error_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error  2.928\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "error = cross_validate_user_modified(test_data_set,user_number,pred_rating,item)\n",
    "print(\"error \",error)\n",
    "print(test_data_set.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K =  1  mae =  0.0  predicted rating  0.0\n"
     ]
    }
   ],
   "source": [
    "%precision %.2f\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "K = 5\n",
    "user_number = 100\n",
    "item = 50\n",
    "predicted_ratings_arr = []\n",
    "error_arr = []\n",
    "mae = math.inf\n",
    "best_k = 0\n",
    "VERBOSE = False\n",
    "\n",
    "if(VERBOSE):\n",
    "    print(\"K\\t Predicted Rating\\t Error\")\n",
    "\n",
    "for k in range(test_data_set.shape[1]):\n",
    "    user = test_data_set[user_number]\n",
    "    pred_rating,top_k = test(user,item,k,values)\n",
    "    error = cross_validate_user_modified(test_data_set,user_number,pred_rating,item)\n",
    "    \n",
    "    error_arr.append(error)\n",
    "    predicted_ratings_arr.append(pred_rating)\n",
    "    \n",
    "    if(mae > error and k!=0):\n",
    "        mae = error\n",
    "        best_k = k\n",
    "    \n",
    "    if(VERBOSE):\n",
    "        print(\"\\n{} \\t{} \\t\\t {}\".format(k,round(pred_rating,2), round(error,2) ))\n",
    "    \n",
    "print(\"Best K = \",best_k,\" mae = \",mae,\" predicted rating \",predicted_ratings_arr[best_k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Error')"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "K_values = range(test_data_set.shape[1])\n",
    "plt.plot(K_values,error_arr,label = \"Accuracy Prediction\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 5 predicted ratings for item ' 50 ' for user ' 100 ' are \n",
      "[0.    0.    0.    0.798 0.825]\n"
     ]
    }
   ],
   "source": [
    "predicted_ratings_df = pd.DataFrame(predicted_ratings_arr)\n",
    "#print(predicted_ratings_df.T)\n",
    "\n",
    "#print(\" error \",error_arr)\n",
    "#sort the ratings\n",
    "idx = np.argsort(error_arr)\n",
    "\n",
    "#print(\"idx \",idx)\n",
    "## get top k similar items\n",
    "top_k = idx[:5]\n",
    "\n",
    "print(\"top 5 predicted ratings for item '\",item,\"' for user '\",user_number,\"' are \")\n",
    "print(np.ravel(predicted_ratings_df.T[top_k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
